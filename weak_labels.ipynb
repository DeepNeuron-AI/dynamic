{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right Ventricle (RV) weak label generation\n",
    "This notebook is a playground to help us develop a set of *weak* labels for RV segmentation from the `echonet` dataset of echocardiograms. This is needed since `echonet` only has expert labels for Left Ventricle (LV) segmentation, and there appears to be no public dataset at all that has RV segmentation labels. Even if our techniques don't work on every video in the dataset, we can just filter out our \"best\" synthetic labels and retrain the segmentation model on them. What we do after that depends on the quality of the trained model..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setting filepaths based on config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import dotenv_values\n",
    "import scipy\n",
    "\n",
    "import echonet\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Directories containing both original and flipped segmentation masks\n",
    "LEFT_SEGMENTATION_DIR = Path(config[\"LEFT_SEGMENT_DIR\"]) # e.g. LEFT_SEGMENT_DIR=\"output/segmentation/all-patients\"\n",
    "LEFT_SEGMENTATION_MASK_DIR = LEFT_SEGMENTATION_DIR / \"segmentation_masks\" # remember that you have to have run the modified segmentation.py to get this segmentations sub-directory!\n",
    "FLIPPED_SEGMENTATION_DIR = Path(config[\"FLIPPED_SEGMENT_DIR\"]) # e.g. FLIPPED_SEGMENT_DIR=\"output/segmentation/flipped\"\n",
    "FLIPPED_SEGMENTATION_MASK_DIR = FLIPPED_SEGMENTATION_DIR / \"segmentation_masks\"\n",
    "\n",
    "ECHONET_VIDEO_DIR = Path(config[\"ECHONET_VIDEO_DIR\"]) # e.g. ECHONET_VIDEO_DIR=\"/home/lex/data/echonet-data/Videos\"\n",
    "\n",
    "# Can assign these colours to numpy arrays so long as the colours are stored in\n",
    "# the last axis of the target array (e.g. image.shape =(112, 112, 3), but not\n",
    "# image.shape = (3, 112, 112)). \n",
    "# Just do image[y_vals, x_vals] = MAGENTA\n",
    "# IMPORTANT: if using within an *opencv* function, you'll want to do \n",
    "# COLOUR.tolist() to convert these to python primitives, else opencv complains about\n",
    "# datatypes\n",
    "# Note also that these are BGR, not RGB, since that's what opencv prefers BGR for historical reasons!\n",
    "RED = np.array([0, 0, 255])\n",
    "GREEN = np.array([0, 255, 0])\n",
    "BLUE = np.array([255, 0, 0])\n",
    "ORANGE = np.array([0, 165, 255])\n",
    "LIGHT_GREY = np.array([211, 211, 211])\n",
    "MAGENTA = np.array([255, 0, 255])\n",
    "YELLOW = np.array([0, 255, 255])\n",
    "WHITE = np.array([255, 255, 255])\n",
    "BLACK = np.array([0, 0, 0])\n",
    "\n",
    "# Just some types for us to use in type hints to make dev easier\n",
    "Point = List[np.intp]\n",
    "Box = Tuple[Point, Point, Point, Point]\n",
    "Rectangle = Tuple[Point, Tuple[float, float], float] # [centre, (width, height), angle]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heights(mask: np.ndarray) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns array of heights of mask for each frame! Height is determined based\n",
    "    on the highest and lowest (in terms of y values) pixels that are True in the\n",
    "    given mask.\n",
    "    \"\"\"\n",
    "    frame_indices, row_indices = np.where(mask.any(axis=1)==True)\n",
    "\n",
    "    heights = []\n",
    "    for frame_index in range(mask.shape[0]):\n",
    "        this_frame = frame_indices == frame_index\n",
    "\n",
    "        if len(row_indices[this_frame]) == 0:\n",
    "            # print(f\"Frame #{frame_index} appears to have no segmentations? Treating this as zero height...\")\n",
    "            heights.append(0)\n",
    "            continue\n",
    "\n",
    "        min_row, max_row = (min(row_indices[this_frame]), max(row_indices[this_frame]))\n",
    "        height = max_row - min_row\n",
    "        heights.append(height)\n",
    "\n",
    "    heights = np.array(heights)\n",
    "    return heights\n",
    "\n",
    "\n",
    "def get_angle(rect: Rectangle) -> float:\n",
    "    \"\"\"\n",
    "    Converts the angle returned by opencv for a rotated rectangle into a more\n",
    "    reliable one that doesn't require you to know which sides correspond to \n",
    "    \"height\" and which correspond to \"width\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angle: float\n",
    "        Angle in degrees in the range of (-90, 90).\n",
    "    \"\"\"\n",
    "    _, (width, height), angle = rect\n",
    "    if width < height:\n",
    "        return 0 - angle\n",
    "    elif height <= width:\n",
    "        return 90 - angle\n",
    "\n",
    "\n",
    "BOTTOM_RIGHT = 0\n",
    "BOTTOM_LEFT = 1\n",
    "TOP_RIGHT = 2\n",
    "TOP_LEFT = 3\n",
    "\n",
    "def find_corner(rect: Rectangle, which: int) -> Point:\n",
    "    \"\"\"\n",
    "    Returns the coordinates of either the \"bottom-right\", \"bottom-left\", \n",
    "    \"top-left\", or \"top-right\" corner of the given (possible rotated) rectangle.\n",
    "    The specific corner is chosen with the `which` parameter.\n",
    "\n",
    "    Note these corner names are a little ambiguous, but were defined based on\n",
    "    what was most applicable to our needs.\n",
    "    \"\"\"\n",
    "    angle = get_angle(rect)\n",
    "    box = np.intp(cv2.boxPoints(rect))\n",
    "\n",
    "    # Critical angle regions are: [-90, -45), [-45, 0], (0, 45), [45, 90) ?\n",
    "    if (-90 <= angle < -45) or (0 < angle < 45):\n",
    "        if which == BOTTOM_LEFT:\n",
    "            return box[3]\n",
    "        elif which == TOP_LEFT:\n",
    "            return box[0]\n",
    "        elif which == TOP_RIGHT:\n",
    "            return box[1]\n",
    "        elif which == BOTTOM_RIGHT:\n",
    "            return box[2]\n",
    "    elif (-45 <= angle <= 0) or (45 <= angle <= 90):\n",
    "        if which == BOTTOM_LEFT:\n",
    "            return box[0]\n",
    "        elif which == TOP_LEFT:\n",
    "            return box[1]\n",
    "        elif which == TOP_RIGHT:\n",
    "            return box[2]\n",
    "        elif which == BOTTOM_RIGHT:\n",
    "            return box[3]\n",
    "\n",
    "\n",
    "def mask_to_image(mask: np.ndarray, max_val: int = 255) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a boolean mask array to a pure black and white image. Useful if\n",
    "    you start with a mask but then want to find contours and perform other image\n",
    "    analysis on that mask    \n",
    "    \"\"\"\n",
    "    return mask.astype(np.uint8) * max_val\n",
    "\n",
    "\n",
    "def image_to_mask(image: np.ndarray, threshold: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a black and white image to a boolean mask, selecting every pixel \n",
    "    whose intensity is **greater than or equal to the threshold**\n",
    "    \"\"\"\n",
    "    return image >= threshold\n",
    "\n",
    "\n",
    "def extrapolate_line(p1: Point, p2: Point, frame_height: int, frame_width: int) -> List[Point]:\n",
    "    \"\"\"\n",
    "    Extends a line between the given pair of points for all given x values and\n",
    "    returns the list of points on this line.\n",
    "    \n",
    "    This clips the points for you in case any of the given x values correspond to\n",
    "    y values outside of the image's borders.\n",
    "    \"\"\"\n",
    "    # Just to be safe, make sure points are in ascending order of x (unless \n",
    "    # vertically alligned)\n",
    "    if p2[0] < p1[0]:\n",
    "        p1, p2 = p2, p1\n",
    "\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "\n",
    "    # Vertical line is special case\n",
    "    if x1 == x2:\n",
    "        # Vertical lines are a special case\n",
    "        ys = np.arange(frame_height)\n",
    "        xs = np.ones(ys.shape, dtype=np.intp) * x1\n",
    "    else:\n",
    "        gradient = (y2 - y1) / (x2 - x1)\n",
    "        y_intercept = y1 - gradient * x1\n",
    "        \n",
    "        # Whether the straight line will be steep or not. If it *is* steep, then that\n",
    "        # means that a single x value can correspond to multiply y values, due to\n",
    "        # pixellated nature of the line, and vice versa if it is *not* steep. We\n",
    "        # therefore need to treat the two steepness cases separately to ensure a\n",
    "        # fully continuous line.\n",
    "        is_steep = abs(y2 - y1) > abs(x2 - x1)\n",
    "        if is_steep:\n",
    "            # Start with all y values, and determine matching xs\n",
    "            ys = np.arange(frame_height)\n",
    "            #    y = mx + c\n",
    "            # => x = (y - c) / m\n",
    "            xs = (ys - y_intercept) / gradient\n",
    "        else:\n",
    "            # Start with all x values, and determine matching ys\n",
    "            xs = np.arange(frame_width)\n",
    "            ys = gradient * xs + y_intercept\n",
    "    \n",
    "    # Make sure coordinates are all corect type for opencv indexing\n",
    "    xs = np.intp(xs)\n",
    "    ys = np.intp(ys)\n",
    "    points = list(zip(xs, ys))\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_min_area_rect(image: np.ndarray) -> Rectangle:\n",
    "    \"\"\"\n",
    "    Finds the largest contour in the given image, and returns the minimum area\n",
    "    rectangle that bounds it.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ((centre_x, centre_y), (width, height), angle)\n",
    "    \"\"\"\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "\n",
    "    # Assume we're only interested in max area contour\n",
    "    areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    max_index = np.argmax(areas)\n",
    "    biggest_contour = contours[max_index]\n",
    "\n",
    "    min_area_rect = cv2.minAreaRect(biggest_contour)\n",
    "\n",
    "    centre, (width, height), angle = min_area_rect\n",
    "    if angle == 90:\n",
    "        # Rotate counterclockwise by 90 degrees if angle is 90 degrees\n",
    "        min_area_rect = (centre, (height, width), 0)\n",
    "\n",
    "    return min_area_rect\n",
    "\n",
    "\n",
    "def get_min_area_box(image: np.ndarray) -> Box:\n",
    "    \"\"\"\n",
    "    Similar to get_min_area_rect(), but instead returns the coordinates of the\n",
    "    box's four corners.\n",
    "    \"\"\"\n",
    "    min_area_rect = get_min_area_rect(image)\n",
    "    if min_area_rect is None:\n",
    "        return None\n",
    "    box = np.intp(cv2.boxPoints(min_area_rect))\n",
    "    \n",
    "    return box\n",
    "\n",
    "\n",
    "def perpendicular_distance_to_line(line: List[List[int]], point: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the perpendicular distance from a straight line to a given point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line: List[List[int]]\n",
    "        `line` is a *pair* of points, where each point contains a single x and y\n",
    "        value.\n",
    "    point: List[int]\n",
    "        A pair of x and y values.\n",
    "    \"\"\"\n",
    "    (x1, y1), (x2, y2) = line\n",
    "    x3, y3 = point\n",
    "\n",
    "    if x2 == x1:\n",
    "        return abs(x1 - x3)\n",
    "\n",
    "    m = (y2 -y1) / (x2 - x1)\n",
    "    a = -1\n",
    "    b = 1 / m\n",
    "    c = x1 - y1 / m\n",
    "\n",
    "    d = abs(a * x3 + b * y3 + c) / math.sqrt(a**2 + b**2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def crop_mask_with_line(mask: np.ndarray, line: List[Point], below=False, above=False, left=False, right=False):\n",
    "    \"\"\"\n",
    "    Sets all pixels to some side(s) of the given line to False in the given \n",
    "    mask, effectively cropping the mask with an arbitrary line.\n",
    "\n",
    "    Note that the given line can technically have points that are out of bounds\n",
    "    of the frame. This is actually needed so we can crop based on particularly\n",
    "    steep lines (e.g. the LV's septum border, which likely only takes up a few\n",
    "    x values in the actual frame). \n",
    "\n",
    "    This modifies the given `mask` in-place.\n",
    "    \"\"\"\n",
    "    mask_height, mask_width = mask.shape\n",
    "\n",
    "    for x, y in line:\n",
    "        y_is_valid = (0 <= y < mask_height)\n",
    "        x_is_valid = (0 <= x < mask_width)\n",
    "\n",
    "        if below and x_is_valid:\n",
    "            y = min(y, mask_height-1)\n",
    "            mask[y:, x] = False\n",
    "        if above and x_is_valid:\n",
    "            y = max(y, 0)\n",
    "            mask[:y, x] = False\n",
    "        if left and y_is_valid:\n",
    "            x = min(x, mask_width-1)\n",
    "            mask[y, :x] = False\n",
    "        if right and y_is_valid:\n",
    "            x = max(x, 0)\n",
    "            mask[y, x:] = False\n",
    "\n",
    "\n",
    "def crop_line_to_frame(line: List[Point], frame_height: int, frame_width: int) -> List[Point]:\n",
    "    \"\"\"\n",
    "    Returns a subset of the given `line` that fits within the given frame \n",
    "    dimensions. This is useful if you have a line that potentially goes beyond \n",
    "    the frame, and you want to show that line using opencv (which will complain\n",
    "    about index errors if any given points are out of bounds).\n",
    "    \"\"\"\n",
    "    # We could probs use cv2.clipLine(), but that only returns the two endpoints,\n",
    "    # so we'd still have to find all points in between anyway!\n",
    "    cropped_line = []\n",
    "    for point in line:\n",
    "        x, y = point\n",
    "        if (0 <= x < frame_width) and (0 <= y < frame_height):\n",
    "            cropped_line.append(point)\n",
    "\n",
    "    return cropped_line\n",
    "\n",
    "\n",
    "def clip_and_draw_line(frame: np.ndarray, line: List[Point], colour: Tuple[int, int, int]):\n",
    "    \"\"\"\n",
    "    Helper function to draw the given line on the given frame, making sure to clip\n",
    "    the line to the frame to avoid any out-of-bounds errors.\n",
    "    \"\"\"\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    img_rect = (0, 0, frame_width, frame_height)\n",
    "    in_bounds, point1, point2 = cv2.clipLine(img_rect, line[0], line[-1])\n",
    "    \n",
    "    cv2.line(frame, point1, point2, colour)\n",
    "\n",
    "\n",
    "def is_a2c(left_segmentation_masks, right_segmentation_masks):\n",
    "    intersection = left_segmentation_masks & right_segmentation_masks\n",
    "    intersection_counts = intersection.sum(axis=(1, 2))\n",
    "    union = left_segmentation_masks | right_segmentation_masks\n",
    "    union_counts = union.sum(axis=(1, 2))\n",
    "    ious = intersection_counts / union_counts\n",
    "    \n",
    "    return np.mean(ious) > 0.5\n",
    "\n",
    "def get_avg_fractional_change(V_area, systoles, diastoles):\n",
    "    # V_area is ventricle area (left or right)\n",
    "    # systoles/diastoles should be the corresponding ventricle's systole/diastole (but in our case we are just using one set)\n",
    "    \n",
    "    avg_systole_area = np.mean([V_area[i] for i in systoles])\n",
    "    avg_diastoles_area = np.mean([V_area[i] for i in diastoles])\n",
    "\n",
    "    return avg_systole_area / avg_diastoles_area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample echonet video and its corresponding raw left and right segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload here so we can just change the VIDEONAME in .env without having to \n",
    "# go back and rerun top cell\n",
    "config = dotenv_values(\".env\")\n",
    "VIDEONAME = config[\"VIDEONAME\"] # e.g. VIDEONAME=\"0X1DB488AC3583E3D6\"\n",
    "\n",
    "LEFT_SEGMENTATION_MASK_FP = LEFT_SEGMENTATION_MASK_DIR / f\"{VIDEONAME}.npy\"\n",
    "FLIPPED_SEGMENTATION_MASK_FP = FLIPPED_SEGMENTATION_MASK_DIR / f\"{VIDEONAME}.npy\"\n",
    "ECHONET_VIDEO_FP = ECHONET_VIDEO_DIR / f\"{VIDEONAME}.avi\"\n",
    "\n",
    "left_segmentation_masks = np.load(LEFT_SEGMENTATION_MASK_FP)\n",
    "flipped_segmentation_masks = np.load(FLIPPED_SEGMENTATION_MASK_FP)\n",
    "right_segmentation_masks = np.flip(flipped_segmentation_masks, -1) # Flip back to original orientation\n",
    "# Remove any pixels in right segmentation that are already in left segmentation\n",
    "# right_segmentation_masks = right_segmentation_masks & ~left_segmentation_masks\n",
    "\n",
    "left_segmentations = mask_to_image(left_segmentation_masks)\n",
    "right_segmentations = mask_to_image(right_segmentation_masks)\n",
    "\n",
    "echonet_video = echonet.utils.loadvideo(str(ECHONET_VIDEO_FP))\n",
    "echonet_video = echonet_video.transpose((1, 2, 3, 0)) # Put colour axis at end for easier colouring of pixels\n",
    "num_frames, frame_height, frame_width, num_channels = echonet_video.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if this is actually an A4C video or if it might be A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Should we be using this video? {not is_a2c(left_segmentation_masks, right_segmentation_masks)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define key variables related to LV\n",
    "These are fine to define once and reuse over and over because we assume the LV segmentations are practically perfect, meaning we will never change them. This is in contrast to the RV segmentations, which we will regularly modify throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LV boxes and rectangles\n",
    "LV_boxes = [get_min_area_box(left_segmentation) for left_segmentation in left_segmentations]\n",
    "LV_rects = [get_min_area_rect(left_segmentation) for left_segmentation in left_segmentations]\n",
    "LV_septum_borders = [[find_corner(rect, BOTTOM_LEFT), find_corner(rect, TOP_LEFT)] for rect in LV_rects]\n",
    "LV_angles = [get_angle(rect) for rect in LV_rects]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim RV segmentation based on LV bounding box\n",
    "If you extend the bottom of the LV's bounding box (which should roughly be in line with the mitral valve, i.e. near-horizontal), then assume that the tricuspid valve is somewhere along that line. Therefore, any of the RV segmentation that appears *below* that line must be incorrectly including some of the RA, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim RV segmentations based on LV bounding box\n",
    "right_segmentations = np.zeros(shape=right_segmentations.shape, dtype=right_segmentations.dtype)\n",
    "\n",
    "valve_cutoff_points_list = []\n",
    "apex_cutoff_points_list = []\n",
    "LV_septum_border_cutoff_points_list = []\n",
    "\n",
    "cutoff_x = np.intp(np.arange(0, frame_width))\n",
    "\n",
    "for i, (LV_box, LV_rect, right_segmentation_mask) in enumerate(zip(LV_boxes, LV_rects, right_segmentation_masks)):\n",
    "    top_left_LV = find_corner(LV_rect, TOP_LEFT)\n",
    "    top_right_LV = find_corner(LV_rect, TOP_RIGHT)\n",
    "    bottom_left_LV = find_corner(LV_rect, BOTTOM_LEFT)\n",
    "    bottom_right_LV = find_corner(LV_rect, BOTTOM_RIGHT)\n",
    "\n",
    "    # Remove any points from RV segmentation that seem to be *below* tricuspid valve (e.g. in RA)\n",
    "    valve_cutoff_points = extrapolate_line(bottom_left_LV, bottom_right_LV, frame_height=frame_height, frame_width=frame_width)\n",
    "    valve_cutoff_points_list.append(valve_cutoff_points)\n",
    "    crop_mask_with_line(mask=right_segmentation_mask, line=valve_cutoff_points, below=True)\n",
    "\n",
    "    # Remove any points from RV segmentation that seem to be *above* heart's apex\n",
    "    apex_cutoff_points = extrapolate_line(top_left_LV, top_right_LV, frame_height=frame_height, frame_width=frame_width)\n",
    "    apex_cutoff_points_list.append(apex_cutoff_points)\n",
    "    crop_mask_with_line(mask=right_segmentation_mask, line=apex_cutoff_points, above=True)\n",
    "\n",
    "    # Remove any points from RV segmentation that seem to be *right* of the LV's inner edge/septum wall\n",
    "    LV_septum_border_cutoff_points = extrapolate_line(bottom_left_LV, top_left_LV, frame_height=frame_height, frame_width=frame_width)\n",
    "    LV_septum_border_cutoff_points_list.append(LV_septum_border_cutoff_points)\n",
    "    crop_mask_with_line(mask=right_segmentation_mask, line=LV_septum_border_cutoff_points, right=True)\n",
    "\n",
    "    right_segmentations[i] = mask_to_image(right_segmentation_mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only select largest contiguous region for each frame's right segmentation.\n",
    "This eliminates small, disconnected blobs that are usually well outside of the RV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include largest contour of right segmentation for each frame. This removes\n",
    "# any smaller, disconnected blobs\n",
    "right_segmentations_copy = right_segmentations.copy()\n",
    "right_segmentations = np.zeros(right_segmentations.shape, dtype=right_segmentations.dtype)\n",
    "for i, right_segmentation in enumerate(right_segmentations_copy):\n",
    "    right_contours, hierarchy = cv2.findContours(right_segmentation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(cnt) for cnt in right_contours]\n",
    "    max_index = np.argmax(areas)\n",
    "    biggest_contour = right_contours[max_index]\n",
    "\n",
    "    biggest_right_segmentation = np.zeros(right_segmentation.shape)\n",
    "    biggest_right_segmentation = cv2.drawContours(biggest_right_segmentation, [biggest_contour], -1, 255, -1)\n",
    "    right_segmentations[i] = biggest_right_segmentation\n",
    "\n",
    "# Update masks too, would be nicer to have these masks and images automatically linked though!\n",
    "right_segmentation_masks = image_to_mask(right_segmentations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine septum width and RV areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RV areas\n",
    "# Since we have already eliminated all but the largest contour, we simply always\n",
    "# use hardcoded index of 0 to access it!\n",
    "# Then extra index zero for contours because of opencv's return values\n",
    "RV_contours_list = [cv2.findContours(frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0][0] for frame in right_segmentations]\n",
    "RV_areas = [cv2.contourArea(contour) for contour in RV_contours_list]\n",
    "\n",
    "# Find septum widths\n",
    "septum_widths = []\n",
    "\n",
    "for i, (right_segmentation, LV_line) in enumerate(zip(right_segmentations, LV_septum_borders)):\n",
    "    RV_rect = get_min_area_rect(right_segmentation)\n",
    "    RV_bottom_right = find_corner(RV_rect, BOTTOM_RIGHT)\n",
    "\n",
    "    x3, y3 = RV_bottom_right\n",
    "    # frame[y3:y3+2, x3:x3+2] = ORANGE\n",
    "    septum_width = perpendicular_distance_to_line(LV_line, RV_bottom_right)\n",
    "    septum_widths.append(septum_width)\n",
    "    # print(f\"Septum: {septum_width}\")\n",
    "\n",
    "print(f\"Mean septum width: {np.mean(septum_widths)} +/- {np.std(septum_widths)}\")\n",
    "RV_boxes = [get_min_area_box(right_segmentation) for right_segmentation in right_segmentations]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove ultrasound borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_ultrasound_corner = (0, 67)\n",
    "top_ultrasound_corner = (61, 6)\n",
    "ultrasound_left_line = extrapolate_line(left_ultrasound_corner, top_ultrasound_corner, frame_height, frame_width)\n",
    "for s in right_segmentation_masks:\n",
    "    crop_mask_with_line(s, ultrasound_left_line, above=True)\n",
    "\n",
    "# Repeat for right side of ultrasound\n",
    "right_ultrasound_corner = (111, 55)\n",
    "top_ultrasound_corner = (62, 6)\n",
    "ultrasound_right_line = extrapolate_line(right_ultrasound_corner, top_ultrasound_corner, frame_height, frame_width)\n",
    "for s in right_segmentation_masks:\n",
    "    crop_mask_with_line(s, ultrasound_right_line, above=True)\n",
    "right_segmentations = mask_to_image(right_segmentation_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove septum from RV segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use average estimated septum width and translate the LV segmentation's inner\n",
    "# edge by that amount to guess the right edge of the RV.\n",
    "mean_septum_width = np.mean(septum_widths)\n",
    "\n",
    "RV_boxes = []\n",
    "RV_lines = []\n",
    "for LV_box, LV_rect, LV_line, right_segmentation_mask, right_segmentation in zip(LV_boxes, LV_rects, LV_septum_borders, right_segmentation_masks, right_segmentations):\n",
    "    LV_angle = get_angle(LV_rect)\n",
    "    \n",
    "    # Since we translate points *left*, and not necessarily perpendicular to LV\n",
    "    # line, we include factor of sin(angle)\n",
    "    translate_x = mean_septum_width * math.cos(math.radians(LV_angle))\n",
    "    RV_line = [point.copy() for point in LV_line]\n",
    "    for point in RV_line:\n",
    "        point[0] -= translate_x\n",
    "\n",
    "    RV_lines.append(RV_line)\n",
    "\n",
    "    RV_box = get_min_area_box(right_segmentation)\n",
    "    # Just in case we need to access this later\n",
    "    RV_boxes.append(RV_box)\n",
    "\n",
    "    # Now remove any pixels in RV segmentation that go beyond its expected inner\n",
    "    # edge\n",
    "    # Note this is a greedier \"right cutoff\" than before, should probs just name this better!\n",
    "    right_cutoff_points = extrapolate_line(RV_line[0], RV_line[1], frame_height=frame_height, frame_width=frame_width)\n",
    "    crop_mask_with_line(right_segmentation_mask, right_cutoff_points, right=True)\n",
    "\n",
    "# Update RV segmentation *images* based on these new masks\n",
    "right_segmentations = mask_to_image(right_segmentation_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace frames with too-small RV with nearby, better segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RV_areas = np.array(RV_areas)\n",
    "mean_RV_area = np.mean(RV_areas)\n",
    "std_RV_area = np.std(RV_areas)\n",
    "\n",
    "z = 1.5\n",
    "small_RV_indices = np.where(mean_RV_area - z * std_RV_area > RV_areas)[0]\n",
    "print(f\"Found {len(small_RV_indices)} frames with suspiciously small area\")\n",
    "\n",
    "RV_areas_copy = RV_areas.copy()\n",
    "for i in small_RV_indices:\n",
    "    offset = 1\n",
    "    while True:\n",
    "        left_i = i - offset\n",
    "        if left_i >= 0 and left_i not in small_RV_indices:\n",
    "            print(f\"Changing frame #{i}'s RV segmentation to that of #{left_i} (offset={offset})\")\n",
    "            right_segmentation_masks[i] = right_segmentation_masks[left_i]\n",
    "            RV_areas[i] = RV_areas_copy[left_i]\n",
    "            break\n",
    "\n",
    "        right_i = i + offset\n",
    "        if right_i < num_frames and right_i not in small_RV_indices:\n",
    "            print(f\"Changing frame #{i}'s RV segmentation to that of #{right_i} (offset={offset})\")\n",
    "            right_segmentation_masks[i] = right_segmentation_masks[right_i]\n",
    "            RV_areas[i] = RV_areas_copy[right_i]\n",
    "            break\n",
    "\n",
    "        offset += 1\n",
    "\n",
    "right_segmentations = mask_to_image(right_segmentation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TAPSE(annulus):\n",
    "    # Find frame with largest RV segmentation, then use its bottom-left corner to do TAPSE stuff...\n",
    "    largest_RV_index = np.argmax(RV_areas)\n",
    "    largest_RV_rect = get_min_area_rect(right_segmentations[largest_RV_index])\n",
    "    # annulus = find_corner(largest_RV_rect, BOTTOM_LEFT)\n",
    "    # annulus = (annulus[0] - 5, annulus[1])\n",
    "    top_right_RV = find_corner(largest_RV_rect, TOP_RIGHT)\n",
    "    top_right_RV = (top_right_RV[0] - 5, top_right_RV[1])\n",
    "    # top_mid_RV = (np.intp(np.mean([top_left_RV[0], top_right_RV[0]])), np.intp(np.mean([top_left_RV[1], top_right_RV[1]])))\n",
    "\n",
    "    TAPSE_line = extrapolate_line(annulus, top_ultrasound_corner, frame_height=frame_height, frame_width=frame_width)\n",
    "\n",
    "    TAPSE_line_cropped = crop_line_to_frame(TAPSE_line, frame_height, frame_width)\n",
    "\n",
    "    TAPSE_cross_sections = []\n",
    "    for frame in echonet_video:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        TAPSE_cross_section = []\n",
    "        for x, y in TAPSE_line_cropped:\n",
    "            # print(frame[y, x])\n",
    "            TAPSE_cross_section.append(frame[y, x])\n",
    "            # echonet_video[0].shape\n",
    "        TAPSE_cross_sections.append(TAPSE_cross_section)\n",
    "\n",
    "    TAPSE_cross_sections = np.array(TAPSE_cross_sections)\n",
    "    TAPSE_cross_sections = TAPSE_cross_sections.transpose((1, 0))\n",
    "    return TAPSE_line, TAPSE_cross_sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify end-systole and end-diastole using LV segmentations\n",
    "Note this code is basically copy-pasted from `echonet-dynamic`'s `segmentation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LV_areas = left_segmentation_masks.sum(axis=(1,2))\n",
    "min_area, max_area = LV_areas.min(), LV_areas.max()\n",
    "trim_min = sorted(LV_areas)[round(len(LV_areas) ** 0.05)]\n",
    "trim_max = sorted(LV_areas)[round(len(LV_areas) ** 0.95)]\n",
    "trim_range = trim_max - trim_min\n",
    "diastoles = scipy.signal.find_peaks(LV_areas, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "systoles = scipy.signal.find_peaks(-LV_areas, distance=20, prominence=(0.50 * trim_range))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE!!! Classify LV and RV by estimated ejection fraction\n",
    "**This is absolutely incorrect lol.**\n",
    "\n",
    "Note we're obviously only using the 2D images, so approximating ESV and EDV with \n",
    "areas, not volumes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we're assuming male patients for simplicity right now, but of course, \n",
    "# these exact numbers depend on the demographics!\n",
    "LV_EF_classifications = {\n",
    "    \"normal\": (52, 72),\n",
    "    \"mildy abnormal\": (41, 51),\n",
    "    \"moderately abnormal\": (30, 40),\n",
    "    \"severely abnormal\": (0, 30),\n",
    "    \"unclassified (very high)\": (73, 100)\n",
    "}\n",
    "# Based on https://www.ahajournals.org/doi/10.1161/CIRCEP.116.004067#:~:text=RV%20dysfunction%20was%20defined%20as,range%2C%207%E2%80%9378%25).\n",
    "RV_EF_classifications = {\n",
    "    \"abnormal\": (0, 45),\n",
    "    \"normal\": (46, 100)\n",
    "}\n",
    "\n",
    "def classify_ejection_fraction(EF: float, classifications: dict) -> str:\n",
    "    # Need to round EF to nearest integer (as percentage) for comparisons\n",
    "    EF = round(EF * 100)\n",
    "    for classification, (lower, upper) in classifications.items():\n",
    "        if lower <= EF <= upper:\n",
    "            return classification\n",
    "\n",
    "for areas, name, classifications in zip((LV_areas, RV_areas), (\"LV\", \"RV\"), (LV_EF_classifications, RV_EF_classifications)):\n",
    "    ave_sys_area = np.mean(areas[systoles])\n",
    "    std_sys_area = np.std(areas[systoles])\n",
    "    ave_dia_area = np.mean(areas[diastoles])\n",
    "    std_dia_area = np.std(areas[diastoles])\n",
    "    print(f\"{name} average areas: EDV={int(ave_dia_area)} ± {int(std_dia_area)}; ESV={int(ave_sys_area)} ± {int(std_sys_area)}   [pixels squared]\")\n",
    "    EF = (ave_dia_area - ave_sys_area) / ave_dia_area\n",
    "    EF_classification = classify_ejection_fraction(EF, classifications)\n",
    "    print(f\"{name} EF = {EF*100:.2f}% ({EF_classification})\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distributions of LV and RV area\n",
    "We would expect the LV distribution to be relatively smooth, and the RV to be a little choppy just due to its poorer segmentation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(14, 5))\n",
    "fig.suptitle(\"Ventricle area distributions\")\n",
    "fig.supxlabel(\"Area [pixels squared]\")\n",
    "fig.supylabel(\"Frequency\")\n",
    "\n",
    "for i, (areas, name, color) in enumerate(zip((LV_areas, RV_areas), (\"LV\", \"RV\"), (\"red\", \"blue\"))):\n",
    "    bins = np.histogram_bin_edges(areas, bins=\"auto\")\n",
    "    axes[i].hist(areas, bins=bins, color=color)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ventricle areas over time\n",
    "If our RV segmentations are decent, we would expect them to correlate with the LV segmentations. A low correlation indicates poor RV segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(14, 5))\n",
    "fig.subplots_adjust(top=0.8, left=0.075)\n",
    "fig.suptitle(\"Ventricle area over time\")\n",
    "fig.supxlabel(\"Frame\")\n",
    "fig.supylabel(\"Area [pixels squared]\")\n",
    "\n",
    "frame_range = np.arange(1, num_frames+1)\n",
    "for i, (areas, name, color) in enumerate(zip((LV_areas, RV_areas), (\"LV\", \"RV\"), (\"red\", \"blue\"))):\n",
    "    axes[i].plot(frame_range, areas, color=color)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].grid()\n",
    "    axes[i].vlines(systoles, *axes[i].get_ylim(), \"magenta\", label=\"Systole\")\n",
    "    axes[i].vlines(diastoles, *axes[i].get_ylim(), \"green\", label=\"Diastole\")\n",
    "    print(axes[i].get_legend_handles_labels())\n",
    "\n",
    "# Both subplots share same legend for systole/diastole lines (thank you: https://stackoverflow.com/a/46921590)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles=handles, labels=labels, loc=(0.45, 0.875), ncol=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ventricle fractional change\n",
    "The function does this by finding the average systole area and dividing that by the average diastole area. This is done for both the LV and RV and currently uses the systole/diastole of the LV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LV_fractional_change = get_avg_fractional_change(LV_areas, systoles, diastoles)\n",
    "RV_fractional_change = get_avg_fractional_change(RV_areas, systoles, diastoles)\n",
    "\n",
    "print(f\"LV fractional area change: {LV_fractional_change}\")\n",
    "print(f\"RV fractional area change: {RV_fractional_change}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average corners of RV segmentation\n",
    "We can then use the average bottom-left corner of the RV as a rough estimate for the annulus' location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RV_boxes = np.array([get_min_area_box(right_segmentation) for right_segmentation in right_segmentations if get_min_area_box(right_segmentation) is not None])\n",
    "RV_points = RV_boxes.reshape((-1, RV_boxes.shape[-1]))\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(RV_points)\n",
    "\n",
    "colours = [\"red\", \"green\", \"violet\", \"orange\"]\n",
    "RV_ave_corners = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, colour in enumerate(colours):\n",
    "    points = RV_points[kmeans.labels_ == i]\n",
    "    xs, ys = zip(*points)\n",
    "    centre_x, centre_y = np.mean(xs), np.mean(ys)\n",
    "    RV_ave_corners.append((centre_x, centre_y))\n",
    "    ax.scatter(xs, ys, color=colour)\n",
    "    ax.plot([centre_x], [centre_y], color=\"black\", marker=\"x\", markersize=15)\n",
    "    ax.set_title(\"RV corners across the entire video\")\n",
    "    ax.set_xlabel(\"x [pixels]\")\n",
    "    ax.set_ylabel(\"y [pixels]\")\n",
    "\n",
    "# Invert axis since y values of images increase as they go down the screen, not up\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Overlay translucent frame from video\n",
    "ax.imshow(echonet_video[diastoles[0]], alpha=0.8)\n",
    "\n",
    "# Save the bottom-left for reuse as the annulus later\n",
    "_bottom_RV_corners = sorted(RV_ave_corners, key=lambda x: x[1])[2:]\n",
    "bottom_left_RV = sorted(_bottom_RV_corners, key=lambda x: x[0])[0]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display segmentation (and whatever else we want) on top of original ultrasound video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annulus = tuple(np.intp(bottom_left_RV))\n",
    "TAPSE_thresh = 40\n",
    "TAPSE_padding = 25\n",
    "print(f\"TAPSE: Starting with annulus={annulus}, threshold={TAPSE_thresh}\")\n",
    "\n",
    "def onclick(event, x, y, flags=None, param=None):\n",
    "    global annulus, TAPSE_cross_sections, TAPSE_line, TAPSE\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(f\"Clicked @ ({x}, {y})\")\n",
    "        annulus = (int(x), int(y))\n",
    "        TAPSE_line, TAPSE_cross_sections = get_TAPSE(annulus=annulus)\n",
    "\n",
    "        # Find intersection of TAPSE line with valve cutoff line to get ballpark\n",
    "        # estimate of annulus height within the TAPSE graph. Crop TAPSE to only\n",
    "        # the estimated annulus region\n",
    "        valve_cutoff_points = valve_cutoff_points_list[0]\n",
    "        shared_pixels = [p for p in valve_cutoff_points if p in TAPSE_line]\n",
    "        if len(shared_pixels) > 0:\n",
    "            shared_pixel = shared_pixels[0]\n",
    "            intersection_index = TAPSE_line.index(shared_pixel)\n",
    "            TAPSE_cross_sections = TAPSE_cross_sections[intersection_index-TAPSE_padding:intersection_index+TAPSE_padding]\n",
    "\n",
    "        # Denoise and threshold TAPSE graph to make it easier to analyse\n",
    "        TAPSE_cross_sections = cv2.fastNlMeansDenoising(TAPSE_cross_sections,None,30,7,21)\n",
    "        TAPSE_cross_sections[TAPSE_cross_sections < TAPSE_thresh] = 0\n",
    "\n",
    "        # Find peaks and troughs of this TAPSE graph\n",
    "        TAPSE_cross_sections_mask = TAPSE_cross_sections > 0\n",
    "        TAPSE_systoles = [np.argmax(TAPSE_cross_sections_mask[:, s]) for s in systoles]\n",
    "        TAPSE_diastoles = [np.argmax(TAPSE_cross_sections_mask[:, d]) for d in diastoles]\n",
    "\n",
    "        TAPSE_cross_sections = cv2.cvtColor(TAPSE_cross_sections, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Highlight the pixels used to determine the TAPSE values\n",
    "        for s, t in zip(systoles, TAPSE_systoles):\n",
    "            TAPSE_cross_sections[t, s] = GREEN.tolist()\n",
    "        for d, t in zip(diastoles, TAPSE_diastoles):\n",
    "            TAPSE_cross_sections[t, d] = RED.tolist()\n",
    "\n",
    "        # Determine final TAPSE estimate\n",
    "        TAPSE = abs(np.mean(TAPSE_systoles) - np.mean(TAPSE_diastoles))\n",
    "        TAPSE_ave_s = int(np.mean(TAPSE_systoles))\n",
    "        TAPSE_ave_d = int(np.mean(TAPSE_diastoles))\n",
    "        p1 = np.array(TAPSE_line[TAPSE_ave_s])\n",
    "        p2 = np.array(TAPSE_line[TAPSE_ave_d])\n",
    "        TAPSE = np.linalg.norm(p1 - p2)\n",
    "\n",
    "def on_change_thresh(new_thresh):\n",
    "    global TAPSE_thresh\n",
    "    TAPSE_thresh = new_thresh\n",
    "    onclick(cv2.EVENT_LBUTTONDOWN, *annulus)\n",
    "\n",
    "def on_change_padding(new_padding):\n",
    "    global TAPSE_padding\n",
    "    TAPSE_padding = new_padding\n",
    "    onclick(cv2.EVENT_LBUTTONDOWN, *annulus)\n",
    "\n",
    "WINDOW = f\"Segmentation: {config['VIDEONAME']}.avi\"\n",
    "cv2.namedWindow(WINDOW, cv2.WINDOW_NORMAL)\n",
    "cv2.setMouseCallback(WINDOW, onclick)\n",
    "\n",
    "cv2.namedWindow(\"TAPSE\", cv2.WINDOW_NORMAL)\n",
    "cv2.createTrackbar(\"TAPSE threshold\", \"TAPSE\", TAPSE_thresh, 255, on_change_thresh)\n",
    "cv2.createTrackbar(\"TAPSE padding\", \"TAPSE\", TAPSE_padding, 40, on_change_padding)\n",
    "\n",
    "# Add help window to show instructions\n",
    "cv2.namedWindow(\"Help\", cv2.WINDOW_NORMAL)\n",
    "help_frame = np.zeros((480, 1440, 3), dtype=np.uint8)\n",
    "cv2.putText(help_frame, \"a = Go back one frame\", org=(40,40), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "cv2.putText(help_frame, \"d = Go forward one frame\", org=(40,100), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "cv2.putText(help_frame, \"space = Pause/play\", org=(40,160), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "cv2.putText(help_frame, \"q = Quit\", org=(40,220), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "cv2.putText(help_frame, \"Left-click on the video to set annulus' position\", org=(40,340), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "# cv2.putText(help_frame, \"annulus' position\", org=(40,400), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=WHITE.tolist(), thickness=2)\n",
    "cv2.imshow(\"Help\", help_frame)\n",
    "\n",
    "i = 0\n",
    "is_playing = True\n",
    "\n",
    "min_y = 1000\n",
    "max_y = -1000\n",
    "\n",
    "# Manually call onclick() to initialise some variables\n",
    "onclick(cv2.EVENT_LBUTTONDOWN, *annulus)\n",
    "try:\n",
    "    while True:\n",
    "        if i >= num_frames:\n",
    "            i = 0\n",
    "        elif i < 0:\n",
    "            i = num_frames - 1\n",
    "\n",
    "        # Copy data for this particular frame\n",
    "        frame = echonet_video[i].copy()\n",
    "        left_segmentation_mask = left_segmentation_masks[i].copy()\n",
    "        left_segmentation = left_segmentations[i].copy()\n",
    "        right_segmentation_mask = right_segmentation_masks[i].copy()\n",
    "        right_segmentation = right_segmentations[i].copy()\n",
    "        LV_box = LV_boxes[i]\n",
    "        RV_box = get_min_area_box(right_segmentation)\n",
    "        \n",
    "        ######## SEGMENTATIONS\n",
    "        frame[left_segmentation_mask] = RED\n",
    "        frame[right_segmentation_mask] = BLUE\n",
    "\n",
    "        ######## BOXES\n",
    "        cv2.drawContours(frame, [LV_box], 0, YELLOW.tolist())\n",
    "        cv2.drawContours(frame, [RV_box], 0, ORANGE.tolist())\n",
    "        \n",
    "        ######## ULTRASOUND BORDERS\n",
    "        # clip_and_draw_line(frame, ultrasound_left_line, GREEN.tolist())\n",
    "        # clip_and_draw_line(frame, ultrasound_right_line, GREEN.tolist())\n",
    "\n",
    "        ######## CUTOFF LINES\n",
    "        LV_septum_border_cutoff_points = LV_septum_border_cutoff_points_list[i]\n",
    "        apex_cutoff_points = apex_cutoff_points_list[i]\n",
    "        valve_cutoff_points = valve_cutoff_points_list[i]\n",
    "        # clip_and_draw_line(frame, LV_septum_border_cutoff_points, ORANGE.tolist())\n",
    "        # clip_and_draw_line(frame, apex_cutoff_points, GREEN.tolist())\n",
    "        # clip_and_draw_line(frame, valve_cutoff_points, MAGENTA.tolist())\n",
    "\n",
    "        ######## TAPSE GRAPH AND LINE\n",
    "        TAPSE_cross_sections_copy = TAPSE_cross_sections.copy()\n",
    "        TAPSE_cross_sections_copy[:, i] = BLUE.tolist()\n",
    "        TAPSE_height = TAPSE_cross_sections_copy.shape[0]\n",
    "        cv2.putText(TAPSE_cross_sections_copy, f\"TAPSE: {TAPSE:.2f}\", org=(5,TAPSE_height-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.25, color=RED.tolist())\n",
    "        cv2.imshow(\"TAPSE\", TAPSE_cross_sections_copy)\n",
    "        clip_and_draw_line(frame, TAPSE_line, RED.tolist())\n",
    "\n",
    "        ######## SHOW CURRENT ANNULUS ESTIMATE ON ORIGINAL FRAME\n",
    "        cv2.circle(frame, annulus, 3, MAGENTA.tolist())\n",
    "        \n",
    "        ######## ADD FRAME COUNTER AT TOP\n",
    "        # top_border = np.zeros((frame_height // 8, frame_width, 3), dtype=frame.dtype)\n",
    "        # top_border[:, :] = np.expand_dims(LIGHT_GREY, (0, 1))\n",
    "        # cv2.putText(top_border, f\"Frame {i+1}/{num_frames}\", org=(5,10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.25, color=RED.tolist())\n",
    "        # frame = np.concatenate([top_border, frame], axis=0)\n",
    "        \n",
    "\n",
    "        ######## SHOW SEGMENTATION AND HANDLE KEYPRESS\n",
    "        cv2.imshow(WINDOW, frame)\n",
    "\n",
    "        keypress = cv2.waitKey(50) & 0xFF\n",
    "        if keypress == ord('q'):\n",
    "            break\n",
    "        elif keypress == ord(' '):\n",
    "            is_playing = not is_playing\n",
    "        elif keypress == ord('a'):\n",
    "            i -= 1\n",
    "        elif keypress == ord('d'):\n",
    "            i += 1\n",
    "        else:\n",
    "            if is_playing:\n",
    "                i += 1\n",
    "\n",
    "        ######### ARRANGE WINDOWS\n",
    "        cv2.moveWindow(WINDOW, 805, 20)\n",
    "        cv2.moveWindow(\"TAPSE\", 70, 525)\n",
    "        cv2.moveWindow(\"Help\", 70, 20)\n",
    "        cv2.resizeWindow(WINDOW, 730, 880)\n",
    "        cv2.resizeWindow(\"TAPSE\", 730, 500)\n",
    "        cv2.resizeWindow(\"Help\", 730, 360)\n",
    "finally:\n",
    "    cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultrasound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f172eeab591fffc7206196735bfc2e29f166a162a3789422c4782f1097623d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
